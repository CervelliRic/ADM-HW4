{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Does basic house information reflect house's description?\n",
    "## Group 11 -  Riccardo Cervelli, Kentaro Kato, Pavan kumar Alikana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AirBnb](https://static.designboom.com/wp-content/uploads/2014/12/peter-pichler-architecture-mirror-houses-designboom-09.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section of  HW4 was perform a clustering analysis of house announcements in Rome from Immobiliare.it. We had to implement two clustering and compare the results we got. We created two datasets and each of them is filled by data that we scraped by https://www.immobiliare.it, but you can check also from  [here](https://www.immobiliare.it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile load_packages.py\n",
    "import time\n",
    "\n",
    "# For webscrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "from lxml import html\n",
    "\n",
    "# For persisting indexes in an external file\n",
    "import pickle\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "# For word tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# For stop words list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For word stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load utility functions to be used through out the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to define all our functions at the beginning of this Section, because is more convenient and more simple to read for you!\n",
    "Here you can chek our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile utilities.py\n",
    "\n",
    "# Common utility functions\n",
    "\n",
    "def extract_number(n):\n",
    "    n = n.replace('.', '')\n",
    "    n = n.replace('+', '')\n",
    "    n_list = [str(s) for s in n.split() if s.isdigit()]\n",
    "\n",
    "    s = ''.join(n_list).strip()\n",
    "    \n",
    "    if not s:\n",
    "        s = 0\n",
    "\n",
    "    return int(s)\n",
    "\n",
    "# Utility functions for reading and writing files using pickel python package\n",
    "def read_file_from_pickle(file):\n",
    "    file_content = {}\n",
    "    \n",
    "    if file.is_file():\n",
    "        with open(file, \"rb\") as f:\n",
    "            file_content = pickle.load(f)\n",
    "            f.close()\n",
    "    \n",
    "    return file_content\n",
    "\n",
    "def write_file_to_pickle(file, content):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(content, f)\n",
    "        f.close()\n",
    "\n",
    "# Apply Jaccard similarity to find out 3 most similar clusters\n",
    "def get_jaccard(a, b):\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "# Utility functions for clustering and wordcloud\n",
    "\n",
    "def get_listing_content(i, sflag):\n",
    "    listing_words = ''\n",
    "    if sflag:\n",
    "        listing_words = listing_content_persist[i]\n",
    "    else:\n",
    "        listing_id = listing_index_persist['listing_ids'][i]\n",
    "        listing_data = listings_persist[listing_id]\n",
    "        listing_words = listing_data['description']\n",
    "\n",
    "    return listing_words + ' '\n",
    "\n",
    "def get_wc_save_path(i, sflag):\n",
    "\n",
    "    f_name_prefix = ''\n",
    "\n",
    "    if(sflag):\n",
    "        f_name_prefix = \"wordcloud\"\n",
    "    else:\n",
    "        f_name_prefix = \"wordcloud_all\"\n",
    "\n",
    "    return f_name_prefix + \"/cluster_\" + str(i)\n",
    "\n",
    "\n",
    "def compare_clusters(c1, c2):\n",
    "\n",
    "    jac_score_list = []\n",
    "    comb_list = []\n",
    "    cmp_output = {}\n",
    "\n",
    "    for i in range(len(c1)):\n",
    "        for j in range(len(c2)):\n",
    "            # Adding the score of each cluster combination to jac_score_list\n",
    "            jac_score_list.append(get_jaccard(set(c1[i]), set(c2[j])))\n",
    "            comb_list.append([i,j])\n",
    "\n",
    "    cmp_output['score_list'] = jac_score_list\n",
    "    cmp_output['comb_list'] = comb_list\n",
    "    \n",
    "    return cmp_output\n",
    "\n",
    "\n",
    "def get_similar_clusters(top_3_list, c1, c2):\n",
    "    similar_clusters = []\n",
    "\n",
    "    for t in top_3_list:\n",
    "        t_list = list(t)\n",
    "        c_list_1 = c1[t_list[1][0]]\n",
    "        c_list_2 = c2[t_list[1][1]]\n",
    "\n",
    "        s_list = list(set(c_list_1 + c_list_2))\n",
    "\n",
    "        similar_clusters.append(s_list)\n",
    "\n",
    "    return similar_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data persisted using pickel library:\n",
    "\n",
    "### We are using the following indexes - \n",
    "\n",
    "- listings.pkl - contains all the listings data as it was scrapped in the dictionary format of \"listing_id\": listing_obj\n",
    "\n",
    "- listing_links.pkl - contains all the individual listing page hyperlinks to be scrapped separately (one time action)\n",
    "\n",
    "- isting_index.pkl - contains all the listing ids(extracted from the listing pages while scrapping), to preserve the ordering of listings while building information and description datasets\n",
    "\n",
    "- listing_content.pkl - contains the data from all individual listings with all the stop words removed, we are using this to find tfidf values of all the words in the vocabulary.pkl file\n",
    "\n",
    "- vocabulary.pkl - contains metadata of the words in the following format (eg: \"123\": [334,4545,645]) where 123 is the word_id taken from words.pkl and 334,4545,645 are listing ids in which that word is present\n",
    "\n",
    "- words.pkl - contains all the words present accross all the listings in the following format (eg: \"house\": \"123\")\n",
    "\n",
    "- iindex_tf_idf.pkl - contains the dictionary of tfidf values of all the words in words.pkl\n",
    "\n",
    "- information_dataset.pkl - Holds the information data set in the format mentioned in the homework text\n",
    "\n",
    "- description_dataset.pkl -  Holds the description data set in the format mentioned in the homework text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile load_data.py\n",
    "\n",
    "# Import all the persisted data at once\n",
    "# Data to import: \n",
    "# Listing data, Individual listing links, Listing Index \n",
    "# Words, Vocabulary, listing_content, iindex_tf_idf\n",
    "# information_dataset, description_dataset\n",
    "\n",
    "# Path to the current working directory to refer to all the files relatively\n",
    "my_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "# Datastructures for holding the listings and other metadata\n",
    "# Please create a directory(in your current working directory) with name 'indexes'  \n",
    "\n",
    "#Holds individual listings data for the listing pages downloaded\n",
    "listings_file = Path(os.path.join(my_path, \"indexes/listings.pkl\"))\n",
    "listings_persist = {}\n",
    "\n",
    "if listings_file.is_file():\n",
    "    with open(listings_file, \"rb\") as listings:\n",
    "        listings_persist = pickle.load(listings)\n",
    "        listings.close()\n",
    "\n",
    "#Holds the URLs of individual listings for extracting complete description of a particular listing\n",
    "listing_links_file = Path(os.path.join(my_path, \"indexes/listing_links.pkl\"))\n",
    "listing_links_persist = read_file_from_pickle(listing_links_file)\n",
    "\n",
    "\n",
    "#Holds the order of individual listing\n",
    "listing_index_file = Path(os.path.join(my_path, \"indexes/listing_index.pkl\"))\n",
    "listing_index_persist = read_file_from_pickle(listing_index_file)\n",
    "     \n",
    "\n",
    "# Retrieving persisted information for listing content and word map (words and vocabulary)\n",
    "content_file = Path(os.path.join(my_path, \"indexes/listing_content.pkl\"))\n",
    "listing_content_persist = read_file_from_pickle(content_file)\n",
    "\n",
    "\n",
    "\n",
    "vocabulary_file = Path(os.path.join(my_path, \"indexes/vocabulary.pkl\"))\n",
    "vocabulary_persist = read_file_from_pickle(vocabulary_file)\n",
    "\n",
    "words_file = Path(os.path.join(my_path, \"indexes/words.pkl\"))\n",
    "words_persist = read_file_from_pickle(words_file)\n",
    "        \n",
    "index_file = Path(os.path.join(my_path, \"indexes/iindex_tf_idf.pkl\"))\n",
    "iindex_tf_idf_persist = read_file_from_pickle(index_file)\n",
    "\n",
    "\n",
    "# Information data set\n",
    "information_ds_file = Path(os.path.join(my_path, \"indexes/information_dataset.pkl\"))\n",
    "information_ds_persist = read_file_from_pickle(information_ds_file)\n",
    "\n",
    "# Description data set - containing tf-idf values\n",
    "description_ds_file = Path(os.path.join(my_path, \"indexes/description_dataset.pkl\"))\n",
    "description_ds_persist = read_file_from_pickle(description_ds_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "In our previuos HW we used Cosine Similarity but now we used Jaccard. We show you how its works in a really simple way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jac](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/jaccard_similariyt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sim](https://i1.wp.com/dataaspirant.com/wp-content/uploads/2015/04/jaccaard2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the files into folder with name data. Every group listing page has information about 25 listings, we downloaded 1000 pages to have a bigger sample.. The website that scraped is [here](https://www.immobiliare.it). In particular, we retrieve more thaa 10k announcements starting from this link. We used the Beautiful Soup library to parse the html file . But we used also time.sleep(t), where t is the number of seconds, to prevent the website block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group listings pages already downloaded\n"
     ]
    }
   ],
   "source": [
    "#%%writefile group_listings_download.py\n",
    "\n",
    "# Please create a directory 'data' \n",
    "# Checking if the pages are already downloaded\n",
    "# Check the first file \n",
    "listing_page_file = Path(os.path.join(my_path, \"data/listing_0.html\"))\n",
    "\n",
    "if listing_page_file.is_file() == False:\n",
    "\n",
    "    # If there are no files then start downloading each html file with a delay of 3 seconds\n",
    "    print('Downloading group listings pages...')\n",
    "    \n",
    "    url_root = 'https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag='\n",
    "\n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "        cur_url = url_root + str(i)\n",
    "\n",
    "        cur_content = requests.get(cur_url)\n",
    "\n",
    "        res_text = BeautifulSoup(cur_content.text, \"lxml\")\n",
    "\n",
    "        cur_html_file= open(\"data/listing_\" + str(i) + \".html\", \"w\")\n",
    "        cur_html_file.write(str(res_text))\n",
    "        cur_html_file.close()\n",
    "        \n",
    "        # 3 seconds delay for the next page download attempt\n",
    "        time.sleep(3)\n",
    "else:\n",
    "    print('Group listings pages already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Download individual listing page to update the full description of the listing, we downloaded 10000 individual listing pages and updated corresponding index file listings.pkl\n",
    "\n",
    "Note: We downloaded the files in to folder with name listing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual listing pages already downloaded\n"
     ]
    }
   ],
   "source": [
    "#%%writefile listings_download.py\n",
    "\n",
    "\n",
    "# Downloading individual listings pages from the listing links persisted\n",
    "# Please create a directory data_detail\n",
    "# Checking if the pages are already downloaded\n",
    "# Check the first file \n",
    "listing_detail_file = Path(os.path.join(my_path, \"data_detail/listing_detail_0.html\"))\n",
    "\n",
    "\n",
    "if listing_detail_file.is_file() == False:\n",
    "    \n",
    "    print('Downloading individual listing pages...')\n",
    "\n",
    "    links_list = []\n",
    "\n",
    "    if(len(listing_links_persist.keys()) != 0):\n",
    "\n",
    "        # Getting listing page links\n",
    "        for key in  listing_links_persist:\n",
    "            cur_link = listing_links_persist[key]\n",
    "\n",
    "            # Check if the link is relative\n",
    "            # If yes make it absolute link\n",
    "            # Need to add better checks here\n",
    "            if(cur_link[0] == \"/\"):\n",
    "                cur_link = \"https://www.immobiliare.it\" + cur_link\n",
    "\n",
    "            links_list.append(cur_link)\n",
    "\n",
    "        # Downloading the pages\n",
    "        for i in range(690, len(links_list)):\n",
    "\n",
    "            cur_url = links_list[i]\n",
    "\n",
    "            cur_content = requests.get(cur_url)\n",
    "\n",
    "            res_text = BeautifulSoup(cur_content.text, \"lxml\")\n",
    "\n",
    "            cur_detail_file = os.path.join(my_path, \"data_detail/listing_detail_\" + str(i) + \".html\")\n",
    "\n",
    "            cur_html_file= open(cur_detail_file, \"w\")\n",
    "            cur_html_file.write(str(res_text))\n",
    "            cur_html_file.close()\n",
    "\n",
    "            # Wait for 3 seconds before downloading the next page\n",
    "            time.sleep(3)\n",
    "else:\n",
    "    print('Individual listing pages already downloaded')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used for web scrapping  BeautifulSoup library  for group listing and individual listing pages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes are already created\n"
     ]
    }
   ],
   "source": [
    "#%%writefile scrape_listings.py\n",
    "\n",
    "# If this is True then the listing description is updated from \n",
    "# the individual pages\n",
    "desc_flag = False\n",
    "\n",
    "if(len(listing_index_persist.keys()) == 0):\n",
    "\n",
    "    print(\"Indexes are being created\")\n",
    "\n",
    "    l_index = 0\n",
    "    listing_index_persist['listing_ids'] = []\n",
    "\n",
    "    # Every page has 25 listings so\n",
    "    # 410*25 will be more than 10000 listings\n",
    "    for i in range(1, 410):\n",
    "\n",
    "        cur_listing_page = BeautifulSoup(open(os.path.join(my_path, 'data/listing_' + str(i) + '.html')), 'html.parser')\n",
    "\n",
    "        listing_container = cur_listing_page.find(class_=\"annunci-list\")\n",
    "        \n",
    "        # Need to improve exception handling in this loop\n",
    "        for cur_listing in listing_container.find_all(class_=[\"listing-item\", \"js-row-detail\"], recursive=False):\n",
    "\n",
    "            listing_dict = {\n",
    "                \"id\": \"\",\n",
    "                \"listing_id\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"price\": 0,\n",
    "                \"locali\": 0,\n",
    "                \"superficie\": 0,\n",
    "                \"bagni\": 0,\n",
    "                \"piano\": 0,\n",
    "                \"immobile\": \"\",\n",
    "                \"listing_link\": \"\",\n",
    "                \"description\": \"\"\n",
    "            }\n",
    "\n",
    "            listing_body = cur_listing.find(class_=\"listing-item_body\")\n",
    "\n",
    "            if(listing_body):\n",
    "\n",
    "                listing_dict['id'] = l_index\n",
    "                listing_dict['listing_id'] = cur_listing.get(\"data-id\")\n",
    "\n",
    "                listing_dict['title'] = listing_body.find(class_=\"titolo\").text.strip()\n",
    "\n",
    "                listing_dict[\"listing_link\"] = listing_body.find(\"a\", {\"id\": \"link_ad_\" + listing_dict['listing_id']}).get(\"href\")\n",
    "\n",
    "                listing_dict['description'] = listing_body.find(class_=\"descrizione\").text.strip()\n",
    "\n",
    "                # Extracting the listing features \n",
    "                listing_features = listing_body.find(class_=[\"listing-features\", \"list-piped\"])\n",
    "\n",
    "                listing_links_persist[listing_dict['listing_id']] = listing_dict[\"listing_link\"]\n",
    "\n",
    "                for cur_feature in listing_features.find_all(class_=\"lif__item\", recursive=False):\n",
    "\n",
    "                    feature_cls_list = cur_feature.get(\"class\")\n",
    "\n",
    "                    # Extract listing price\n",
    "                    if 'lif__pricing' in feature_cls_list:\n",
    "                        listing_dict['price'] = extract_number(cur_feature.text.strip())\n",
    "                    else:\n",
    "                        # Extract other features information\n",
    "                        # @TODO: Need to refine locali to contain a list: example: 1-5 should be [1,2,3,4,5]\n",
    "                        feature_name = cur_feature.find(class_=\"lif--muted\")\n",
    "\n",
    "                        # @TODO: Need to do this more efficiently\n",
    "                        if(feature_name):\n",
    "                            feature_name = feature_name.text.strip()\n",
    "\n",
    "                            if feature_name in listing_dict:\n",
    "                                feature_value = cur_feature.find(class_=\"text-bold\").text.strip()\n",
    "                                listing_dict[feature_name] = extract_number(feature_value)\n",
    "\n",
    "\n",
    "                listing_index_persist['listing_ids'].append(listing_dict['listing_id'])\n",
    "\n",
    "                l_index += 1\n",
    "                listings_persist[listing_dict['listing_id']] = listing_dict\n",
    "\n",
    "\n",
    "    # Remove duplicate listing entries\n",
    "    listing_index_persist['listing_ids'] = list(set(listing_index_persist['listing_ids']))\n",
    "\n",
    "    # Persist the listings object and dictionary using pickel library\n",
    "    \n",
    "    #Save listings data\n",
    "    write_file_to_pickle(listings_file, listings_persist)\n",
    "\n",
    "    #Save individual listings links data\n",
    "    write_file_to_pickle(listing_links_file, listing_links_persist)\n",
    "\n",
    "    #Save index of listings\n",
    "    write_file_to_pickle(listing_index_file, listing_index_persist)\n",
    "\n",
    "else:\n",
    "    print(\"Indexes are already created\")\n",
    "\n",
    "\n",
    "#print(\"No of links:\")\n",
    "#print(len(listing_links_persist.keys()))\n",
    "\n",
    "#print(\"No of listings:\")\n",
    "#print(len(listings_persist.keys()))\n",
    "\n",
    "#print(\"No of listings in the listing index file:\")\n",
    "#print(len(listing_index_persist['listing_ids']))\n",
    "\n",
    "\n",
    "if desc_flag:\n",
    "\n",
    "    # Parse the detail pages \n",
    "    # And update the description of individual listings\n",
    "    for i in range(len(listing_links_persist.keys())):\n",
    "        cur_detail_page = BeautifulSoup(open(os.path.join(my_path, 'data_detail/listing_detail_' + str(i) + '.html')), 'html.parser')\n",
    "\n",
    "        cur_page_contact = cur_detail_page.find('div',{\"id\":\"up-contact-box\"})\n",
    "        if cur_page_contact:\n",
    "            cur_page_elem = cur_page_contact.find(class_=\"info-agenzia\")\n",
    "\n",
    "            if cur_page_elem:\n",
    "                cur_page_id = cur_page_elem.get(\"data-annuncio\")\n",
    "\n",
    "                cur_page_description =  cur_detail_page.find(class_=\"description-text\")\n",
    "\n",
    "                if cur_page_description:\n",
    "                    cur_page_description = cur_page_description.text.strip()\n",
    "\n",
    "                    cur_page_description = \"\".join(cur_page_description.splitlines())\n",
    "\n",
    "                    if cur_page_id in listings_persist:\n",
    "                        listings_persist[cur_page_id]['description'] = cur_page_description\n",
    "                    else:\n",
    "                        pass\n",
    "                        #print(\"Page key not found in the persisted data\")\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(\"Page Description not found\")\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Page ID not found\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Contact not found\")\n",
    "\n",
    "    #Save listings data with new content (complete listing description)\n",
    "    write_file_to_pickle(listings_file, listings_persist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finally we can show you our Datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information data set already present\n",
      "9987\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_information_ds.py\n",
    "\n",
    "#Preparing information data set\n",
    "if(len(information_ds_persist.keys()) == 0):\n",
    "\n",
    "    information_ds_persist['dataset'] = []\n",
    "\n",
    "    # Get the persisted listings data\n",
    "    for listing_id in listing_index_persist['listing_ids']:\n",
    "        cur_listing = listings_persist[listing_id]\n",
    "\n",
    "        listing_info = [cur_listing['price'], cur_listing['locali'], cur_listing['superficie'], cur_listing['bagni'], cur_listing['piano']]\n",
    "\n",
    "        information_ds_persist['dataset'].append(listing_info)\n",
    "    \n",
    "    #Save information data set\n",
    "    write_file_to_pickle(information_ds_file, information_ds_persist)\n",
    "\n",
    "else:\n",
    "    print(\"Information data set already present\")\n",
    "\n",
    "\n",
    "print(len(information_ds_persist['dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Description \n",
    "Our description DataFrame is almost 600 MB storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary:\n",
    "Here we are creating the vocabulary dictionary, firstly we create a dictionary, then for each word in the doc's description and title, we check if the word is already in the dictionary, if not we add it to and we assign it an id, if it is in the dictionary we skip and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vamsigunturi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Vocabulary data set already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_vocabulary.py\n",
    "\n",
    "#First we import stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "#To remove punctuation we use regexptokenizer, but we leave dollar symbol $ because maybe is used in some queries\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$')\n",
    "#we create the stemmer\n",
    "ps = SnowballStemmer('italian')\n",
    "\n",
    "list_len = len(listing_index_persist['listing_ids'])\n",
    "\n",
    "if(len(listing_content_persist.keys()) == 0):\n",
    "    \n",
    "    listing_word_map = {}\n",
    "    \n",
    "    # We reach here if we don't have indexes already present\n",
    "    print(\"Vocabulary is being created...\")\n",
    " \n",
    "    for i in range(list_len):\n",
    "        \n",
    "        cur_list_id = listing_index_persist['listing_ids'][i]\n",
    "        \n",
    "        cur_list_obj = listings_persist[cur_list_id]\n",
    "\n",
    "        # Extract all the text in the individual listing\n",
    "        # For listing title\n",
    "        t1 = cur_list_obj['title']\n",
    "        \n",
    "        # For listing content\n",
    "        t2 = cur_list_obj['description']\n",
    "        \n",
    "        t = t1+ ' ' +t2\n",
    "        t = t.lower()\n",
    "        t = tokenizer.tokenize(t)\n",
    "        \n",
    "        # This array will contain all the valid words in a given review after removing \n",
    "        # all the stop words, punctuations, stemming etc..,, we will use this information\n",
    "        # to find out the term frequency there by tf-idf values\n",
    "        listing_words = []\n",
    "        \n",
    "        for r in t :\n",
    "            if not r in stop_words:\n",
    "                sr = r #ps.stem(r) - avoid stemming for now for the wordcloud\n",
    "                \n",
    "                listing_words.append(sr)\n",
    "                \n",
    "                if not  sr in listing_word_map:\n",
    "                    listing_word_map[sr] = [i]\n",
    "                else:\n",
    "                    listing_word_map[sr]+=[i]\n",
    "                    \n",
    "                    \n",
    "        listing_content_persist[i] = ' '.join(listing_words)\n",
    "    \n",
    "    # Saving the content and indexes for the first time\n",
    "    # We made use of pickel python module\n",
    "    #Saving content dictionary\n",
    "    write_file_to_pickle(content_file, listing_content_persist)\n",
    "    \n",
    "    # Word and Vocabulary indexes based on word map\n",
    "    c = 0\n",
    "    for key in listing_word_map:\n",
    "        words_persist[key] = c\n",
    "        vocabulary_persist[c] = listing_word_map[key]\n",
    "        c += 1\n",
    "    \n",
    "    #Save vocabulary and words\n",
    "    write_file_to_pickle(vocabulary_file, vocabulary_persist)\n",
    "    write_file_to_pickle(words_file, words_persist)\n",
    "else:\n",
    "    print(\"Vocabulary data set already present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tfidfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Indexes already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile calculate_tfidfs.py\n",
    "\n",
    "if(len(iindex_tf_idf_persist.keys()) == 0):\n",
    "    \n",
    "    print(\"Inverted Indexes are being calculated\")\n",
    "\n",
    "    word_iindex = {}\n",
    "\n",
    "    #Creating inverted index using tf-idf and consine similarity\n",
    "    for word in words_persist:\n",
    "        word_doc_list = vocabulary_persist[words_persist[word]]\n",
    "        word_iindex[word] = []\n",
    "\n",
    "        # Store indexes based on number of times a particular word is present in a given document\n",
    "        for doc in word_doc_list:\n",
    "            doc_content = listing_content_persist[doc]\n",
    "            # Pushing the term frequency with document id\n",
    "            word_iindex[word].append([doc, doc_content.split().count(word)])\n",
    "\n",
    "    # Store indexes based on tf-idf\n",
    "    docs_length = len(listing_content_persist.keys())\n",
    "    iindex_tf_idf_persist = word_iindex\n",
    "\n",
    "    for key, word in iindex_tf_idf_persist.items():\n",
    "        # find out the relative importance of a particular terms relating it to document count\n",
    "        idf= math.log10( docs_length / len(word) )\n",
    "\n",
    "        for elem in word:\n",
    "            # Add the document score corresponding to a particular term which we then use in the \n",
    "            # search results ranking of documents\n",
    "            elem[1] = idf * elem[1]\n",
    "    \n",
    "\n",
    "    # Persisting the indexes calculated \n",
    "    write_file_to_pickle(index_file, iindex_tf_idf_persist)\n",
    "else:\n",
    "    print(\"Inverted Indexes already present\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the inverted index file. Firstly we create a dictionary, then for each word in the doc's description and title, we check if the word is in the dictionary and if not we add its id to the dictionary and we add the doc name to the list (value) of the word. Instead if it is already in the dictionary we add the doc number to the list of the word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create description dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description data set already present\n"
     ]
    }
   ],
   "source": [
    "#%%writefile create_description_ds.py\n",
    "\n",
    "# Preparing Description data set\n",
    "# Create description data set\n",
    "# Extract the words in individual listings\n",
    "# Create a matrix with rows as listings and columns as words\n",
    "# Combile listing title and description\n",
    "# Remove stop words\n",
    "# Calculate the term frequency\n",
    "# Calculate the td*idf score for that word in that document\n",
    "# Which gives the description data set for the 10000 listings saved\n",
    "if(len(description_ds_persist.keys()) == 0):\n",
    "\n",
    "    print(\"Description data set is being created...\")\n",
    "\n",
    "    list_len = len(listing_index_persist['listing_ids'])\n",
    "    description_ds = []\n",
    "    \n",
    "    #Build the description data set\n",
    "    for i in range(list_len):\n",
    "        \n",
    "        cur_list_id = listing_index_persist['listing_ids'][i]\n",
    "        \n",
    "        cur_list_obj = listings_persist[cur_list_id]\n",
    "\n",
    "        cur_word_list = []\n",
    "        \n",
    "        #Initialize each word tf-idf with 0's\n",
    "        for word in words_persist:\n",
    "            cur_word_list.append(0)\n",
    "\n",
    "        # @TODO: Need to optimize the number of verfications done here\n",
    "        for key, word in iindex_tf_idf_persist.items():\n",
    "            # elem[0] - list_id\n",
    "            # elem[1] - tf-idf\n",
    "            for elem in word:\n",
    "                # Update tf-idf of that word for that listing \n",
    "                if(elem[0] == i):\n",
    "                    cur_word_list[words_persist[key]] = elem[1]\n",
    "        \n",
    "        description_ds.append(cur_word_list)\n",
    "\n",
    "    description_ds_persist['dataset'] = description_ds\n",
    "\n",
    "\n",
    "    # Persisting the indexes calculated \n",
    "    write_file_to_pickle(description_ds_file, description_ds_persist)\n",
    "    \n",
    "else:\n",
    "    print(\"Description data set already present\")\n",
    "    \n",
    "#print(description_ds_persist['dataset'][789])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply Elbow method to find optimal number of clusters for both Information and Description dataset created above. You can find more information about Elbow Method if you [click here](https://en.wikipedia.org/wiki/Elbow_method_(clustering))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile apply_elbow_method.py\n",
    "\n",
    "def apply_elbow(dataset):\n",
    "\n",
    "    wcss = []\n",
    "\n",
    "    \n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "        kmeans.fit(dataset)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    return wcss\n",
    "\n",
    "\n",
    "# Applying elbow method for information data set\n",
    "score_list_ids = apply_elbow(information_ds_persist['dataset'])\n",
    "\n",
    "\n",
    "# Applying elbow method for description data set\n",
    "#score_list_dds = apply_elbow(description_ds_persist['dataset'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start assigning clusters from 5 groups because assigning just one cluster does not make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot for IDS: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1acb644160>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Information dataset Elbow\n",
    "\n",
    "# From 2 clusters \n",
    "print(\"Plot for IDS: \")\n",
    "\n",
    "plt.plot(range(2,11), score_list_ids[1:])\n",
    "plt.plot([2, 10],[score_list_ids[1], score_list_ids[9]])\n",
    "plt.plot(5, score_list_ids[4], 'o', ms=30, mec='r', mfc='none', mew=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the distance from the line on graph between the sum of squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.0,\n",
       " 3: 2.409235392148073,\n",
       " 4: 3.0669757621545966,\n",
       " 5: 3.2807786449700878,\n",
       " 6: 2.905069474442201,\n",
       " 7: 2.3550966377111573,\n",
       " 8: 1.7019726564859285,\n",
       " 9: 0.881872833069561,\n",
       " 10: 0.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = {}\n",
    "\n",
    "p1=np.array([2,score_list_ids[1]])\n",
    "p2=np.array([10,score_list_ids[9]])\n",
    "\n",
    "for i in range(1,10):\n",
    "    p3 = np.array([i+1,score_list_ids[i]])\n",
    "    d = abs(np.cross(p2-p1,p3-p1)/np.linalg.norm(p2-p1))\n",
    "    distances[i+1] = d\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you can see, the optimal number of clusters is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Comparison among cluster\n",
    "\n",
    "### Cluster the listings in information and description dataset by using the optimal cluster count obtained from the Elbow method\n",
    "\n",
    "### Find similar clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering for Information Dataset: \n",
      "Clustering for Description Dataset: \n",
      "Applying Jacard similarity for the clusters: \n",
      "[(0.6629667003027245, [0, 1]), (0.2554699537750385, [2, 1]), (0.05429723859757989, [3, 1])]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile cluster_listings.py\n",
    "\n",
    "# data - is the dataset\n",
    "# k  - number of clusters\n",
    "# @TODO: Need to use Elbow method to decide on\n",
    "# Optimal number of clusters\n",
    "\n",
    "def cluster_documents(data, k):   \n",
    "    \n",
    "    #use k-means to clusterize the songs\n",
    "    kmeans = KMeans(n_clusters=k, init='random') # initialization\n",
    "    kmeans.fit(data) # actual execution\n",
    "    c = kmeans.predict(data)\n",
    "    c_list = list(c)\n",
    "\n",
    "    clustered_list = []\n",
    "\n",
    "    # Creating a multi dimentional array based on k\n",
    "    for c in range(k):\n",
    "        clustered_list.append([])\n",
    "\n",
    "    # Extract the listing ids from indexes\n",
    "    index = 0\n",
    "    for i in c_list:\n",
    "        clustered_list[i].append(index)\n",
    "        index += 1\n",
    "    \n",
    "    return clustered_list\n",
    "\n",
    "# @TODO: Based on the optimial cluster count from Elbow method\n",
    "print('Clustering for Information Dataset: ')\n",
    "ids_c_list = cluster_documents(information_ds_persist['dataset'], 5)\n",
    "\n",
    "\n",
    "print('Clustering for Description Dataset: ')\n",
    "dds_c_list = cluster_documents(description_ds_persist['dataset'], 5)\n",
    "\n",
    "# Jaccard similarity\n",
    "print('Applying Jacard similarity for the clusters: ')\n",
    "cmp_output  = compare_clusters(ids_c_list, dds_c_list)\n",
    "\n",
    "\n",
    "# Getting top 3 similar clusters and use it to generate wordcloud\n",
    "top_3_tuple = sorted(zip(cmp_output['score_list'], cmp_output['comb_list']), reverse=True)[:3]\n",
    "top_3_list = list(top_3_tuple)\n",
    "print(top_3_list)\n",
    "\n",
    "#Preparing the cluster list for generating word cloud\n",
    "top_c_list = get_similar_clusters(top_3_list, ids_c_list, dds_c_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 3 similar clusters between information dataset and description dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Word cloud of house descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile create_wordcloud.py\n",
    "\n",
    "def create_wordcloud(clist, stopwords_flag):\n",
    "\n",
    "    c_index = 0   \n",
    "    \n",
    "    for cluster in clist:\n",
    "        \n",
    "        cur_cluster_words = \" \"\n",
    "        \n",
    "        # Extracting all the words of the listings in current cluster\n",
    "        for list_id in cluster:\n",
    "            cur_cluster_words +=  get_listing_content(list_id, stopwords_flag)\n",
    "        \n",
    "        #strg_cloud = ' '.join(strg_cloud.split())\n",
    "        \n",
    "        wordcloud = WordCloud(width = 300, height = 300, margin = 0, collocations=False).generate(cur_cluster_words)\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.margins(x=0,y=0)\n",
    "        plt.savefig(get_wc_save_path(c_index, stopwords_flag))\n",
    "        #plt.show()\n",
    "\n",
    "        c_index += 1  \n",
    "\n",
    "\n",
    "#Creating wordcloud with top 3 similar clusters\n",
    "# Wordcloud with all the words\n",
    "create_wordcloud(top_c_list, False)\n",
    "\n",
    "# Wordcloud without stopwords\n",
    "create_wordcloud(top_c_list, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of wordcloud:\n",
    "\n",
    "### We applied wordcloud to 2 sets of data, \n",
    "#### 1) without stopwords \n",
    "#### 2) with stopwords(all words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Without stopwords:\n",
    "\n",
    "\n",
    "### Cluster 1\n",
    "\n",
    "![title](wordcloud/cluster_0.png)\n",
    "\n",
    "\n",
    "### Cluster 2\n",
    "\n",
    "![title](wordcloud/cluster_1.png)\n",
    "\n",
    "\n",
    "### Cluster 3\n",
    "\n",
    "![title](wordcloud/cluster_2.png)\n",
    "\n",
    "\n",
    "## With all the words:\n",
    "\n",
    "\n",
    "### Cluster 1\n",
    "\n",
    "![title](wordcloud_all/cluster_0.png)\n",
    "\n",
    "\n",
    "### Cluster 2\n",
    "\n",
    "![title](wordcloud_all/cluster_1.png)\n",
    "\n",
    "\n",
    "### Cluster 3\n",
    "\n",
    "![title](wordcloud_all/cluster_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![immobiliare.it](https://www.progedil90.it/blog/wp-content/uploads/2015/12/crescita-mercato-immobiliare-italia-2015.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Find the duplicates! \n",
    "\n",
    "### Welcome to the second part of this HomeWork!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section of HW4 was define a hash function that associates a value to each string to check whether there are some duplicate strings.We have two different ways,that we'll explain you later..\n",
    "Now we wanna try to explain what does hashing algorithm really mean.\n",
    "The point of a hash value is that it is theoretically impossible to find the original input number without knowing the hashing algorithm used to create the eventual hash.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HASSSHH](https://blog.cex.io/wp-content/uploads/2014/10/Hashing-Algorhitm_Blog1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE GOOAL!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps that you need to perform:\n",
    "- Convert the string containing the password to a (potentially large) number\n",
    "- Use a hash function to map the number to a large range.\n",
    "- find if two numbers fall on the same range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is our file??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000000 ? one hundred and ten million lines? are we kidding?\n"
     ]
    }
   ],
   "source": [
    "# we can check how many rows this file have.\n",
    "lunghezza=len(password)\n",
    "print(lunghezza,'?','one hundred and ten million lines? are we kidding?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Huge](https://media.makeameme.org/created/what-is-that-5b3c88.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('passwords2.txt')as file: # read file .txt \n",
    "    password = file.readlines()\n",
    "for count in range(len(password)):\n",
    "    password[count] = password[count].replace(\"\\n\",\"\")# for each line we have replaced \"\\n\" with \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we opened the passwords2.txt file, but only in Reading mode, we red the file line for line, where the lines were strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I° option, order doesn't matter..\n",
    "-  Associate a number to strings,  but in this case the  order is not important.For istance , \"AABA\" = \"AAAB\" , these strings will have the same number. \n",
    "\n",
    "### II° option, order is important..\n",
    "-  Associate a number to strings,  but now the  order IS IMPORTANT.For istance , \"AABA\" != \"AAAB\" now will have the different numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with the first case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without order\n",
    "def MYHASH1(password):\n",
    "    count = 1\n",
    "    for elem in password:\n",
    "        count = (count  ^ (ord(elem)*(7*37**17-27) << 2))\n",
    "    count % (2**128)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101052929264617799440078480769  is equal to  101052929264617799440078480769 ?\n",
      "True\n",
      "101052929264617799440078480769  is different to  101052929264617799440078480769 ?\n",
      "False\n",
      "39589707798715011771259137633  and  668205925723153104309090066721  are the same ?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print((MYHASH1('ciao')),' is equal to ',MYHASH1('icao'),'?')\n",
    "print(MYHASH1('oaic')== MYHASH1('ciao'))\n",
    "print((MYHASH1('oaic')),' is different to ',MYHASH1('ciao'),'?')\n",
    "print(MYHASH1('oaic')!= MYHASH1('ciao'))\n",
    "print((MYHASH1('CIAONE')),' and ',MYHASH1('CIAONE2'),' are the same ?')\n",
    "print(MYHASH1('CIAONE')== MYHASH1('CIAONE2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some tests to verify that our algorithm works...\n",
    "Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improve no collisions probability !\n",
    "Quite often the above mentioned polynomial hash is good enough, and no collisions will happen during tests with a small number of strings.  What if we compared a strings\n",
    "with 110000000 million of different strings?.Now the probability increase, it is pretty much guaranteed that this task will end will a collision and returns the wrong result.\n",
    "But we found a really easy trick to get better probabilities. We can just compute two different hashes for each string (by using two different P, and/or different m, and compare these pairs instead. If for istance, at the beginning the probability that collisions appear was ${10}^{-9}$ now will be ${10}^{-18}$\n",
    "\n",
    "Different strings with each other (e.g. by counting how many unique strings exists), then the probability of at least one collision happening is already ≈1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Double_(password):\n",
    "    dicto = dict()\n",
    "    coll = 0 \n",
    "    double= 0\n",
    "    with open('passwords2.txt', 'r') as f: \n",
    "        for i, line in enumerate(f):\n",
    "            check = password(line)\n",
    "            if check in dicto.keys():\n",
    "                double += 1 \n",
    "                if sorted(dicto[check]) != sorted(line):\n",
    "                    coll += 1\n",
    "            else:\n",
    "                dicto[check] = line\n",
    "    f.close()\n",
    "    return (coll,double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll,double = Double_(MYHASH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 0 collisions and 10000000 duplicates\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", coll,'collisions','and', double,'duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty good, 10 M duplicates and no Collisions,just as we expected.\n",
    "We used a Collision resistance Algorithm , ‘cryptographic hash function’ H are:\n",
    "\n",
    "- Collision resistance: It is hard to find two messages $x≠y$\n",
    "$x ≠ y $ such that $H(x)=H(y)$ $H(x)=H(y)$.Some people call this ‘strong collision resistance’, but this terminology is not widespread.\n",
    "- Preimage resistance: Given a hash h\n",
    ", it is hard to find a message x\n",
    " such that $H(x)=h$\n",
    "$H(x)=h$, i.e. such that x is a preimage under $H$ of $h$.\n",
    "- Second-preimage resistance: Given a message x\n",
    "it is hard to find a second message $y ≠ x$ $y ≠ x$ such that $H(x)=H(y)$ $H(x)=H(y)$, i.e. that x is a second preimage under H of $H(y)$ $H(y)$ distinct from y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyze the second case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$hash(s)=s[0]+s[1]⋅P+s[2]⋅P2+...+s[n−1]⋅Pn−1modm=∑i=0n−1s[i]⋅Pi$\n",
    "where P\n",
    " and m\n",
    " are some chosen, positive numbers. It is called a polynomial rolling hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MYHASH2(password):\n",
    "    p = 151\n",
    "    \n",
    "    m = 14484968830081\n",
    "    hash_value = 0\n",
    "    p_pow = 1\n",
    "    for i, char in enumerate(password):\n",
    "        hash_value = (hash_value + ord(char) * p_pow) % m\n",
    "        p_pow = (p_pow * p) % m\n",
    "    \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384395212  is equal to  384394312 ?\n",
      "False\n",
      "343261012  is different to  384395212 ?\n",
      "True\n",
      "5457512639981  and  4269370026710  are the same ?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print((MYHASH2('ciao')),' is equal to ',MYHASH2('icao'),'?')\n",
    "print(MYHASH2('oaic')== MYHASH2('ciao'))\n",
    "print((MYHASH2('oaic')),' is different to ',MYHASH2('ciao'),'?')\n",
    "print(MYHASH2('oaic')!= MYHASH2('ciao'))\n",
    "print((MYHASH2('CIAONE')),' and ', MYHASH2('CIAONE2'),' are the same ?')\n",
    "print(MYHASH2('CIAONE')== MYHASH2('CIAOEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with order\n",
    "m = [0] * 14484968830081\n",
    "\n",
    "coll = 0\n",
    "double = 0\n",
    "\n",
    "for i in password:\n",
    "    num_ = MYHASH2(i)\n",
    "    index = num_ % 14484968830081\n",
    "    if m[index] == 0:\n",
    "        m[index] = [i]\n",
    "    elif i in m[index]:\n",
    "        double += 1\n",
    "    else:\n",
    "        m[index].append(i)\n",
    "        coll += 1\n",
    "print(coll,double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 0 collisions and 5000000 duplicates\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", coll,'collisions','and', double,'duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Conclusion(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you were able to find two different strings that created the same output (a collision) .\n",
    "Then, at some factory during the process, you would be inputting two different strings and that particular factory would return the same output for both lines.\n",
    "But this means that you have found a collision at one factory, and this is a contradiction !For any hashing algorithm to be successful, it needs to be easy to create an output but impossible to find an input. In other words, in this homework we converted each string to number. For istance if you know that the hash value is 144 and the algorithm used was to multiple the input number by 12, it becomes easy to find that the original input number was 12. On the other hand, imagine if the input number could be a list of numbers that are added together before they’re multiplied by 12. This means that the input numbers could now be [12], [2,4,6], [3,3,3,3], or any other list of numbers that adds up to 12. With this hashing function, it becomes much more difficult to find the original input number when the hash algorithm and hash value are all you have.\n",
    "We implemented our hash Functions to find the duplicates in both cases.In the first case we discovered 10M duplicates and no collisions, because we used a Collision resistance Algorithm. in the second case, we found 5M duoplicates, the half of before,and this was rather obvious because now we know that each string \"SHOULD\" have a different number. As you can see also for the second part we have 0 collisions, and this is very good, but we know it because our algorithm is collision resistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
